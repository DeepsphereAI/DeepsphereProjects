{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from autocorrect import spell\n",
    "\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/omar/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "sconfig = tf.ConfigProto()\n",
    "# sconfig.gpu_options.per_process_gpu_memory_fraction = 0.45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 25\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10000\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def process_str(string, bot_input=False, bot_output=False):\n",
    "    string = string.strip().lower()\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`:]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = string.split(\" \")\n",
    "    string = [re.sub(r\"[0-9]+\", \"NUM\", token) for token in string]\n",
    "    string = [stemmer.stem(re.sub(r'(.)\\1+', r'\\1\\1', token)) for token in string]\n",
    "    string = [spell(token).lower() for token in string]\n",
    "    # Truncate string\n",
    "    while True:\n",
    "        try:\n",
    "            string.remove(\"\")\n",
    "        except:\n",
    "            break\n",
    "    if(not bot_input and not bot_output):\n",
    "        string = string[0:MAX_LEN]\n",
    "    elif(bot_input):\n",
    "        string = string[0:MAX_LEN-1]\n",
    "        string.insert(0, \"</start>\")\n",
    "    else:\n",
    "        string = string[0:MAX_LEN-1]\n",
    "        string.insert(len(string), \"</end>\")\n",
    "    old_len = len(string)\n",
    "    for i in range((MAX_LEN) - len(string)):\n",
    "        string.append(\" </pad> \")\n",
    "    string = re.sub(\"\\s+\", \" \", \" \".join(string)).strip()\n",
    "    return string, old_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10407\n"
     ]
    }
   ],
   "source": [
    "data = cPickle.load(open(\"all_convos.pkl\", \"rb\"))\n",
    "print(len(data))\n",
    "user = [item[0] for item in data]\n",
    "bot = [item[1] for item in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(os.path.isfile(\"user_processed.pkl\")):\n",
    "    user = cPickle.load(open(\"user_processed.pkl\", \"rb\"))\n",
    "else:\n",
    "    user = [process_str(item) for item in user]\n",
    "    cPickle.dump(user, open(\"user_processed.pkl\", \"wb\"))\n",
    "\n",
    "if(os.path.isfile(\"bot_in_processed.pkl\")):\n",
    "    bot_inputs = cPickle.load(open(\"bot_in_processed.pkl\", \"rb\"))\n",
    "else:\n",
    "    bot_inputs = [process_str(item, bot_input=True) for item in bot]\n",
    "    cPickle.dump(bot_inputs, open(\"bot_in_processed.pkl\", \"wb\"))\n",
    "\n",
    "if(os.path.isfile(\"bot_out_processed.pkl\")):\n",
    "    bot_outputs = cPickle.load(open(\"bot_out_processed.pkl\", \"rb\"))\n",
    "else:\n",
    "    bot_outputs = [process_str(item, bot_output=True) for item in bot]\n",
    "    cPickle.dump(bot_outputs, open(\"bot_out_processed.pkl\", \"wb\"))\n",
    "    \n",
    "    \n",
    "user_lens = np.array([message[1] for message in user]).astype(np.int32)\n",
    "user = np.array([message[0] for message in user])\n",
    "\n",
    "bot_inp_lens = np.array([message[1] for message in bot_inputs]).astype(np.int32)\n",
    "bot_out_lens = np.array([message[1] for message in bot_outputs]).astype(np.int32)\n",
    "\n",
    "bot_inputs = np.array([message[0] for message in bot_inputs])\n",
    "bot_outputs = np.array([message[0] for message in bot_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show statistics about length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average user message: 10.602959546459115, average bot message: 13.784087633323724\n",
      "80th percentile of user lengths: 17.0, 80th percentile of bot lengths: 25.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Average user message: {}, average bot message: {}\".format(np.mean(user_lens), np.mean(bot_inp_lens)))\n",
    "print(\"80th percentile of user lengths: {}, 80th percentile of bot lengths: {}\".format(np.percentile(user_lens, 80), np.percentile(bot_inp_lens, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow = CountVectorizer()\n",
    "\n",
    "bow.fit(user.tolist() + bot_inputs.tolist())\n",
    "vocab = list(bow.vocabulary_.keys())\n",
    "vocab.insert(0, \"NUM\")\n",
    "vocab.insert(0, \"UNK\")\n",
    "vocab.insert(0, \"</end>\")\n",
    "vocab.insert(0, \"</start>\")\n",
    "vocab.insert(0, \"</pad>\")\n",
    "cPickle.dump(vocab, open(\"vocab\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_ph = tf.placeholder(user.dtype, name=\"user_placeholder\")\n",
    "bot_inp_ph = tf.placeholder(bot_inputs.dtype, name=\"bot_inp_placeholder\")\n",
    "bot_out_ph = tf.placeholder(bot_outputs.dtype, name=\"bot_out_placeholder\")\n",
    "\n",
    "user_lens_ph = tf.placeholder(user_lens.dtype, shape=[None], name=\"user_len_placeholder\")\n",
    "bot_inp_lens_ph = tf.placeholder(bot_inp_lens.dtype, shape=[None], name=\"bot_inp_lens_placeholder\")\n",
    "bot_out_lens_ph = tf.placeholder(bot_out_lens.dtype, shape=[None], name=\"bot_out_lens_placeholder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_user = tf.data.Dataset.from_tensor_slices(user_ph)\n",
    "tf_bot_inp = tf.data.Dataset.from_tensor_slices(bot_inp_ph)\n",
    "tf_bot_out = tf.data.Dataset.from_tensor_slices(bot_out_ph)\n",
    "\n",
    "tf_user_lens = tf.data.Dataset.from_tensor_slices(user_lens_ph)\n",
    "tf_bot_inp_lens = tf.data.Dataset.from_tensor_slices(bot_inp_lens_ph)\n",
    "tf_bot_out_lens = tf.data.Dataset.from_tensor_slices(bot_out_lens_ph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data/Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"), tf.name_scope(\"data\"):\n",
    "    words = tf.contrib.lookup.index_table_from_tensor(mapping=tf.constant(vocab), default_value=3)\n",
    "    inverse = tf.contrib.lookup.index_to_string_table_from_tensor(mapping=tf.constant(vocab), default_value=\"UNK\", name=\"inverse_op\")\n",
    "\n",
    "    tf_user = tf_user.map(lambda string: tf.string_split([string])).map(lambda tokens: (words.lookup(tokens)))\n",
    "    tf_bot_inp = tf_bot_inp.map(lambda string: tf.string_split([string])).map(lambda tokens: (words.lookup(tokens)))\n",
    "    tf_bot_out = tf_bot_out.map(lambda string: tf.string_split([string])).map(lambda tokens: (words.lookup(tokens)))\n",
    "    \n",
    "    data = tf.data.Dataset.zip((tf_user, tf_bot_inp, tf_bot_out, tf_user_lens, tf_bot_inp_lens, tf_bot_out_lens))\n",
    "    data = data.shuffle(buffer_size=256).batch(BATCH_SIZE)\n",
    "    data = data.prefetch(10)\n",
    "    data_iterator = tf.data.Iterator.from_structure(data.output_types, data.output_shapes,\n",
    "                                                   None, data.output_classes)\n",
    "    train_init_op = data_iterator.make_initializer(data, name='dataset_init')\n",
    "    user_doc, bot_inp_doc, bot_out_doc, user_len, bot_inp_len, bot_out_len = data_iterator.get_next()\n",
    "    user_doc = tf.sparse_tensor_to_dense(user_doc)\n",
    "    bot_inp_doc = tf.sparse_tensor_to_dense(bot_inp_doc)\n",
    "    bot_out_doc = tf.sparse_tensor_to_dense(bot_out_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"embedding\"):\n",
    "    embedding = tf.get_variable(\"embedding\", [len(vocab), 200], initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "    embedded_user = tf.nn.embedding_lookup(embedding, user_doc)\n",
    "    embedded_user_dropout = tf.nn.dropout(embedded_user, 0.7)\n",
    "    \n",
    "    embedded_bot_inp = tf.nn.embedding_lookup(embedding, bot_inp_doc)\n",
    "    embedded_bot_inp_dropout = tf.nn.dropout(embedded_bot_inp, 0.7)\n",
    "    \n",
    "    embedded_user_dropout = tf.reshape(embedded_user_dropout, [-1, MAX_LEN, 200])\n",
    "    embedded_bot_inp_dropout = tf.reshape(embedded_bot_inp_dropout, [-1, MAX_LEN, 200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"encoder\"):\n",
    "    # Build RNN cell\n",
    "    encoder_GRU = tf.nn.rnn_cell.GRUCell(128)\n",
    "    encoder_cell_fw = tf.nn.rnn_cell.DropoutWrapper(encoder_GRU, input_keep_prob=0.7, \n",
    "                                                 output_keep_prob=0.7, state_keep_prob=0.9)\n",
    "    \n",
    "    encoder_cell_bw = tf.nn.rnn_cell.DropoutWrapper(encoder_GRU, input_keep_prob=0.7, \n",
    "                                                 output_keep_prob=0.7, state_keep_prob=0.9)\n",
    "    encoder_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        encoder_cell_fw, encoder_cell_bw, embedded_user_dropout,\n",
    "        sequence_length=user_len, dtype=tf.float32)\n",
    "    encoder_state = tf.concat(encoder_state, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection layer (Output of decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"projection\"):\n",
    "    projection_layer = tf.layers.Dense(\n",
    "    len(vocab), use_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"decoder\"):\n",
    "    decoder_GRU = tf.nn.rnn_cell.GRUCell(256)\n",
    "    decoder_cell = tf.nn.rnn_cell.DropoutWrapper(decoder_GRU, input_keep_prob=0.7, \n",
    "                                                 output_keep_prob=0.7, state_keep_prob=0.9)\n",
    "    # Helper for use during training\n",
    "    # During training we feed the decoder\n",
    "    # the target sequence\n",
    "    # However, during testing we use the decoder's\n",
    "    # last output\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        embedded_bot_inp_dropout, bot_inp_len)\n",
    "    # Decoder\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell, helper, encoder_state,\n",
    "        output_layer=projection_layer)\n",
    "    # Dynamic decoding\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "    logits = outputs.rnn_output\n",
    "    translations = outputs.sample_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss computation normalized by batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(bot_out_doc,\n",
    "                                                                    [-1, MAX_LEN]), logits=logits)\n",
    "    mask = tf.sequence_mask(bot_out_len, dtype=tf.float32)\n",
    "    train_loss = (tf.reduce_sum(loss * mask) / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam with gradient clipping and learning rate scheduling using cosine decay + restarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Adam'):\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "    learning_rate = tf.train.cosine_decay_restarts(0.001, global_step, 550, t_mul=1.1)\n",
    "    adam_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    adam_gradients, v = zip(*adam_optimizer.compute_gradients(train_loss))\n",
    "    adam_gradients, _ = tf.clip_by_global_norm(adam_gradients, 10.0)\n",
    "    adam_optimize = adam_optimizer.apply_gradients(zip(adam_gradients, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"inference\"):\n",
    "    # Helper\n",
    "    # Start token is 1, which is the </start> token\n",
    "    # End token is 2\n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "        embedding,\n",
    "        tf.fill([BATCH_SIZE], 1), 2)\n",
    "\n",
    "    # Decoder\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell, helper, encoder_state,\n",
    "        output_layer=projection_layer)\n",
    "    # Dynamic decoding\n",
    "    test_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder, maximum_iterations=10)\n",
    "    test_translations = tf.identity(test_outputs.sample_id, name=\"word_ids\")\n",
    "    test_words = tf.identity(inverse.lookup(tf.cast(test_translations, tf.int64)), name=\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A function for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testBot(sess):\n",
    "    text = [\"Hello\"] + [\"\"] * (BATCH_SIZE - 1)\n",
    "    num_text = len(text)\n",
    "    text = [process_str(sentence) for sentence in text]\n",
    "    text_len = np.array([item[1] for item in text]).astype(np.int32)\n",
    "    text = np.array([item[0] for item in text])\n",
    "    \n",
    "    user_test_ph = tf.placeholder(text.dtype)\n",
    "    user_test_lens_ph = tf.placeholder(text_len.dtype)\n",
    "    \n",
    "    tf_user_test = tf.data.Dataset.from_tensor_slices(user_test_ph).map(lambda string: tf.string_split([string])).map(lambda tokens: (words.lookup(tokens)))\n",
    "    tf_user_test_lens = tf.data.Dataset.from_tensor_slices(user_test_lens_ph)\n",
    "    \n",
    "    test_data = tf.data.Dataset.zip((tf_user_test, tf_bot_inp, tf_bot_out,\n",
    "                                     tf_user_test_lens, tf_bot_inp_lens, tf_bot_out_lens))\n",
    "    \n",
    "    test_data = test_data.batch(num_text).prefetch(1)\n",
    "    test_init_op = data_iterator.make_initializer(test_data)\n",
    "    \n",
    "    sess.run(test_init_op, feed_dict={\n",
    "        user_test_ph: user,\n",
    "        bot_inp_ph: bot_inputs[0:num_text],\n",
    "        bot_out_ph: bot_outputs[0:num_text],\n",
    "        user_test_lens_ph: user_lens,\n",
    "        bot_inp_lens_ph: bot_inp_lens[0:num_text],\n",
    "        bot_out_lens_ph: bot_out_lens[0:num_text]\n",
    "    })\n",
    "    translations_text = sess.run(inverse.lookup(tf.cast(test_translations, tf.int64)))\n",
    "    return translations_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('Loss', train_loss)\n",
    "    tf.summary.scalar('LR', learning_rate)\n",
    "    merged = tf.summary.merge_all()\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_vis = config.embeddings.add()\n",
    "    embedding_vis.tensor_name = embedding.name\n",
    "    vocab_str = '\\n'.join(vocab)\n",
    "    metadata = pd.Series(vocab)\n",
    "    metadata.name = \"label\"\n",
    "    metadata.to_csv(\"checkpoints/metadata.tsv\", sep=\"\\t\", header=True, index_label=\"index\")\n",
    "    embedding_vis.metadata_path = 'metadata.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training\n",
      "Epoch 0: Loss(Mean): 69.88444519042969 Loss(Std): 15.55992603302002\n",
      "[b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK']\n",
      "Epoch 1: Loss(Mean): 63.92498016357422 Loss(Std): 6.699320316314697\n",
      "[b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK']\n",
      "Epoch 2: Loss(Mean): 62.49559020996094 Loss(Std): 7.132476329803467\n",
      "[b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'have' b'UNK' b'UNK']\n",
      "Epoch 3: Loss(Mean): 61.721195220947266 Loss(Std): 6.579246520996094\n",
      "[b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'</end>' b'UNK' b'UNK']\n",
      "Epoch 4: Loss(Mean): 58.166316986083984 Loss(Std): 6.813504695892334\n",
      "[b'UNK' b'UNK' b'have' b'UNK' b'</end>' b'UNK' b'</end>' b'UNK' b'</end>'\n",
      " b'UNK']\n",
      "Epoch 5: Loss(Mean): 54.21249008178711 Loss(Std): 6.547247409820557\n",
      "[b'UNK' b'</end>' b'UNK' b'have' b'UNK' b'num' b'num' b'star' b'hotel'\n",
      " b'UNK']\n",
      "Epoch 6: Loss(Mean): 52.51726531982422 Loss(Std): 6.151824474334717\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'to' b'num' b'star' b'hotel' b'UNK']\n",
      "Epoch 7: Loss(Mean): 50.343624114990234 Loss(Std): 6.309841156005859\n",
      "[b'UNK' b'have' b'UNK' b'num' b'num' b'num' b'star' b'hotel' b'UNK'\n",
      " b'</end>']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-dfba33bf832a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madam_optimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/omar/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/omar/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/omar/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/omar/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/omar/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/omar/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "print(\"Started training\")\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_dir = 'checkpoints/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "save_path = os.path.join(save_dir, 'best_validation')\n",
    "\n",
    "sess = tf.InteractiveSession(config=sconfig)\n",
    "\n",
    "writer = tf.summary.FileWriter('./checkpoints', sess.graph)\n",
    "projector.visualize_embeddings(writer, config)\n",
    "\n",
    "\n",
    "sess.run([words.init, tf.global_variables_initializer(), inverse.init])\n",
    "step = 0\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    if(i % 10 == 0):\n",
    "        saver.save(sess=sess, save_path=save_path, write_meta_graph=True)\n",
    "    sess.run(train_init_op, feed_dict={\n",
    "        user_ph: user,\n",
    "        bot_inp_ph: bot_inputs,\n",
    "        bot_out_ph: bot_outputs,\n",
    "        user_lens_ph: user_lens,\n",
    "        bot_inp_lens_ph: bot_inp_lens,\n",
    "        bot_out_lens_ph: bot_out_lens\n",
    "    })\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            _, batch_loss, summary = sess.run([adam_optimize, train_loss, merged])\n",
    "            writer.add_summary(summary, i)\n",
    "            losses.append(batch_loss)\n",
    "        except tf.errors.InvalidArgumentError:\n",
    "            continue\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Epoch {}: Loss(Mean): {} Loss(Std): {}\".format(i, np.mean(losses), np.std(losses)))\n",
    "            losses = []\n",
    "            break\n",
    "        sess.run(inc_gstep)\n",
    "        step += 1\n",
    "    print(testBot(sess)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
